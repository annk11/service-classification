# Sarcasm - Analysis
Репозиторий содержит обученную BERT-модель (`ruBERT tiny`), обученную на текстовых данных.

## Code

Предобученная модель находится здесь: [HuggingFace](https://huggingface.co/cointegrated/rubert-tiny).

Для обучения используется обычный Trainer.

Для развертывания модели используется Pipeline и FastAPI, стандартный порт `1234`.

## Expertiments

Используемые ресурсы:
- Win10
- CPU
- 16 RAM

Данные:
- дисбаланса нет
- много ненужных символов

Какие модели обучали:
- Logistic Regression
- Naive Bayes
- Random Forest
- XGBoost
- ruBERT

Предобработка и валидация:
- По стандарту делим датасет на отложенную и обучающую в соотношении `0.8/0.2`.
- В языковых моделях используем всю выборку и проверяем уже на тестовой при валидации в трейнере.
- В качестве метрики - `F1`, так как классы сбалансированы.

Результат:
- Самая эффективная модель `ruBERT-tiny`

## Deploy and inference

Собранный образ основан на базовом Python. Новый образ нужно собрать с помощью `build`.

Для инференса используем Docker-compose:

```bash
docker compose up --build -d
```

После чего можно споконо заходить на Swagger через порт `1234`.